{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch basics and Linear Regression\n",
    "### Back -propogation : Differences between Gradient descent, Stochiastic gradient descent and Batch gradient descent with examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch basics"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Difference between numpy and tensor\n",
    "Pytorch tensors work with GPU (can do large number of matrix operation than CPU)\n",
    "To work with GPU , we need to write code in pgmming language CUDA (similar to C). Need to move data from CPU to GPU.\n",
    "Tensors are written in CUDA language which runs on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opt for Auto complete while coding\n",
    "import rlcompleter, readline\n",
    "readline.parse_and_bind('tab:complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = torch.tensor([[[2,3],[3,4]] , [[2,5],[4,7]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2, 3],\n",
       "         [3, 4]],\n",
       "\n",
       "        [[2, 5],\n",
       "         [4, 7]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ar1 = np.array([[[2,4],[3,5]],[[6,3],[1,2]]])\n",
    "ar = np.array([[[2,4],[3,5]],[[6,3],[1,2,6]]]) #This doesnt give error even though the last matrix has 3 elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 2 at dim 2 (got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-58d75306c38d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#This  give error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 2 at dim 2 (got 3)"
     ]
    }
   ],
   "source": [
    "mat = torch.tensor([[[2,4],[3,5]],[[6,3],[1,2,6]]]) #This  gives error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(3., requires_grad=True)\n",
    "u = torch.tensor(4., requires_grad=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Derivative - Deals with numbers\n",
    "Gradients - Deals with matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = u + w\n",
    "print(y)\n",
    "y.backward()\n",
    "#derivative\n",
    "print(\"dy/dw = \" , w.grad )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1.,2.])\n",
    "z = torch.tensor(x)  #convert Array to tensor\n",
    "print(type(z))\n",
    "y = torch.from_numpy(x) #convert Array to tensor\n",
    "print(type(y))\n",
    "m = y.numpy()  #convert tensor to ndarray\n",
    "print(type(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.is_cuda # This tensor is not stored in GPu, it is stored in CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(3, dtype=torch.float32)\n",
    "#torch.tensor(3, dtype='float32')#gives error\n",
    "p = np.array(3, dtype='float32')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Matrix multiplication = Dot product =  Element-wise multiplication and then sums the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication = dot_product using @ tensor(5)\n",
      "Elemet-wise multiplication using * tensor([3, 2])\n",
      "Matrix multiplication = dot_product using dot method tensor(5)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor([1,2])\n",
    "x2 = torch.tensor([3,1])\n",
    "z = x1 @ x2\n",
    "print(\"Matrix multiplication = dot_product using @\",z)\n",
    "\n",
    "p = x1*x2\n",
    "print(\"Elemet-wise multiplication using *\",p)\n",
    "\n",
    "q = torch.dot(x1,x2)\n",
    "print(\"Matrix multiplication = dot_product using dot method\", q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression using basic tensor operations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Shape = [No of rows, No. of columns]\n",
    "\n",
    "We need to predict the yield of apples and oranges -> Targetshape =  (number of example , no of targets) -> [5,2]\n",
    "\n",
    "Features are Temp,rainfall,humidity -> Inputshape = (No of examples , no of parameters(features)) -> [5,3]\n",
    "\n",
    "Weight matrix = For each target, each feature across all examples we need to assign weights. i.e weights are same for all examples.\n",
    "\n",
    "Shape of Weight matrix = [No of targets , No of input features]  = [2,3]\n",
    "\n",
    "Bias term -> When all the input features are zero, output need not be zero, there will some small value in the output. This value is set by bias term. Bias term is also shared among all the examples. Hence, we will have 1 bias term for one target output. \n",
    "\n",
    "Shape of bias matrix = [No. of targets , 1] = [2,1]\n",
    "\n",
    "yield_apple  = w11 * temp + w12 * rainfall + w13 * humidity + b1\n",
    "yield_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2\n",
    "\n",
    "Logic: Randomly initialise weight matrix and bias matrix. We will be changing this weight matrix to get low loss[actualVal - predictedVal]. Hence we would take grads for w and b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3)\n"
     ]
    }
   ],
   "source": [
    "# 5 examples, each example gives Temp,rainfall,humidity values. Eg: [73(temp),67(rainfall),43(humidity)]\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "print(inputs.shape)\n",
    "x = torch.from_numpy(inputs) #convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2)\n",
      "Range of target  22.0  -  133.0\n"
     ]
    }
   ],
   "source": [
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')\n",
    "print(targets.shape)\n",
    "y = torch.from_numpy(targets)\n",
    "print(\"Range of target \",np.min(targets),\" - \" , np.max(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8033,  0.1748,  0.0890],\n",
      "        [-0.6137,  0.0462, -1.3683]], requires_grad=True)\n",
      "tensor([0.3375, 1.0111], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Initialise weight matrix with random values. Values picked from Normal distribution, usually ranging from -1 to +1, mean=0 and std deviation =1\n",
    "torch.manual_seed(3)\n",
    "w =torch.randn(2,3, requires_grad=True)\n",
    "#w = torch.empty((1,3)).normal_(mean=0,std=1) #Other way of initialising the weights\n",
    "b =torch.randn(2,requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* performs element-wise multiplication\n",
    "@ performs matrix multiplication\n",
    "x.numel() -> give number of elements in a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    yhat = x @ w.t() + b\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted matrix  tensor([[  74.5165,  -99.5312],\n",
      "        [  94.5155, -138.3418],\n",
      "        [  98.8109, -125.5529],\n",
      "        [  93.0817, -110.2279],\n",
      "        [  78.7760, -132.6801]], grad_fn=<AddBackward0>)\n",
      "Actual target  tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "yhat = model(x)\n",
    "print(\"Predicted matrix \", yhat)\n",
    "print(\"Actual target \", y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Calculate loss -> MSE\n",
    "> y - yhat\n",
    "> sqr = square(y - yhat)\n",
    "> sum(all elements in sqr) / totalElements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function : Cost function mse \n",
    "def loss_mse(y,yhat):\n",
    "    diff = y- yhat\n",
    "    sqr = diff*diff  # * -> elementwise multiplication\n",
    "    mse = torch.sum(sqr)/sqr.numel()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIAL LOSS BEFORE APPLYING GRADIET DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL LOSS BEFORE APPLYING GRADIET DESCENT\n",
      "Average Loss tensor(24446.6367, grad_fn=<DivBackward0>)\n",
      "Average loss in each element  tensor(156.3542, grad_fn=<SqrtBackward>)\n",
      "On average, each element in the prediction differs from the actual target by about tensor(156.3542, grad_fn=<SqrtBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"INITIAL LOSS BEFORE APPLYING GRADIET DESCENT\")\n",
    "loss = loss_mse(y,yhat)\n",
    "print(\"Average Loss\", loss)\n",
    "elementwise_loss = torch.sqrt(loss)\n",
    "print(\"Average loss in each element \",elementwise_loss)\n",
    "print(\"On average, each element in the prediction differs from the actual target by about\", elementwise_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute gradients - 1 ITERATION/1 EPOCH"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "dL/dW -> How loss changes with unit increase/decrease in weights\n",
    "dL/db ->Rate of change in loss or slope of loss w.r.t w and b / How loss changes with unit increase/decrease in bias\n",
    "\n",
    "Intuition: Optimist\n",
    "1. If slope(gradient) w.r.t w1 is positive,  as w1 increases, loss increase.\n",
    "    Hence, decrease w1 to reduce loss\n",
    "2. If slope(gradient) w.r.t w1 is Negative,  as w1 increases, loss decrease.\n",
    "    Hence, increase w1 to reduce loss\n",
    "\n",
    "Goal is to find set of weights, where loss is lowest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8033,  0.1748,  0.0890],\n",
      "        [-0.6137,  0.0462, -1.3683]], requires_grad=True)\n",
      "tensor([[  1280.8098,     91.1297,    284.9165],\n",
      "        [-17806.6328, -19511.7695, -12133.7656]])\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "wgrad = w.grad\n",
    "print(wgrad)\n",
    "# Inference\n",
    "# For every unit change in w11, loss increases by 1280.80. To reduce the loss, we need to subtract w11 by 1280.80\n",
    "# For every unit change in w12, loss increases by 91.129\n",
    "# For every unit change in w21, loss decreased by 17806.6. To reduce the loss, we need to add w21 with 17806.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3375, 1.0111], requires_grad=True)\n",
      "tensor([  11.7401, -213.2668])\n"
     ]
    }
   ],
   "source": [
    "print(b)\n",
    "bgrad = b.grad\n",
    "print(bgrad)\n",
    "# Inference\n",
    "# For every 1 unit increase in b1, loss increases by 11.74\n",
    "# For every 1 unit increase in b1, loss decreases by -213.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights tensor([[ 0.8033,  0.1748,  0.0890],\n",
      "        [-0.6137,  0.0462, -1.3683]], requires_grad=True)\n",
      "Initial bias tensor([0.3375, 1.0111], requires_grad=True)\n",
      "Updated weights  tensor([[ 0.7905,  0.1739,  0.0861],\n",
      "        [-0.4357,  0.2413, -1.2469]], requires_grad=True)\n",
      "Updated bias matrix tensor([0.3374, 1.0132], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Update the weights and biases matrix\n",
    "# While updating the weights, we should not modify the gradients.Hence use, torch.no_grad\n",
    "# While updating, we multiply grad with small number(learning rate - alpha), to ensure that we dont modify weights by large number.\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"Initial weights\",w)\n",
    "    print(\"Initial bias\",b)\n",
    "    w-=wgrad * 1e-5\n",
    "    b-=bgrad * 1e-5\n",
    "    print(\"Updated weights \", w)\n",
    "    print(\"Updated bias matrix\", b)\n",
    "    wgrad.zero_()\n",
    "    bgrad.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOSS AFTER PERFORMING BACK_PROPOGATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output tensor([[ 73.3979, -68.2398],\n",
      "        [ 93.0874, -97.1996],\n",
      "        [ 97.4091, -76.8757],\n",
      "        [ 91.6305, -79.1834],\n",
      "        [ 77.6052, -93.1664]], grad_fn=<AddBackward0>)\n",
      "Average Loss tensor(16736.2539, grad_fn=<DivBackward0>)\n",
      "Average loss in each element  tensor(129.3687, grad_fn=<SqrtBackward>)\n",
      "On average, each element in the prediction differs from the actual target by about tensor(129.3687, grad_fn=<SqrtBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss with these new weights and biases\n",
    "yhat = model(x)\n",
    "print(\"Predicted output\",yhat)\n",
    "loss = loss_mse(y,yhat)\n",
    "print(\"Average Loss\", loss)\n",
    "elementwise_loss = torch.sqrt(loss)\n",
    "print(\"Average loss in each element \",elementwise_loss)\n",
    "print(\"On average, each element in the prediction differs from the actual target by about\", elementwise_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESULT INTERPRETATION after applying GRADIENT DESCENT/BACK-PROPOGATION ONCE\n",
    "After first iteration loss reduced from 24446.6367 to 16736.255\n",
    "\n",
    "After first iteration loss exhibited by each element reduced from 156.34 to 129.36, i.e each predicted element differs from actual value by 129.36 on an avg\n",
    "\n",
    "1st iteration -> Takes all the 5 examples(x), calculates yhat, loss, grads and updates the weights. \n",
    "2nd iterations -> Takes all the 5 examples(x), calculates yhat, loss, grads and updates the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent : Consider all examples at each epoch\n",
    "### PERFOM BACK_PROPOGATION 100 times i.e epochs=100, Considering all examples in each epoch \n",
    "100 ITERATIONS = 100 EPOCHS , CONSIDERING ALL EXAMPLES IN EACH EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iter  0  =  tensor(24446.6367, grad_fn=<DivBackward0>)\n",
      "Loss at iter  10  =  tensor(1017.9881, grad_fn=<DivBackward0>)\n",
      "Loss at iter  20  =  tensor(671.6713, grad_fn=<DivBackward0>)\n",
      "Loss at iter  30  =  tensor(601.9100, grad_fn=<DivBackward0>)\n",
      "Loss at iter  40  =  tensor(544.7454, grad_fn=<DivBackward0>)\n",
      "Loss at iter  50  =  tensor(494.2185, grad_fn=<DivBackward0>)\n",
      "Loss at iter  60  =  tensor(449.4562, grad_fn=<DivBackward0>)\n",
      "Loss at iter  70  =  tensor(409.7740, grad_fn=<DivBackward0>)\n",
      "Loss at iter  80  =  tensor(374.5703, grad_fn=<DivBackward0>)\n",
      "Loss at iter  90  =  tensor(343.3152, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Train for 100 epoch / say 100 iterations\n",
    "# Hyperparameters ::  number of epoch(here,we assigned 100) and learning rate(here, we assigned 1e-5)\n",
    "for i in range(100):\n",
    "    yhat = model(x)\n",
    "    loss = loss_mse(y,yhat)\n",
    "    if(i % 10==0):\n",
    "        print(\"Loss at iter \" ,i ,\" = \",loss)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w-=w.grad * 1e-5\n",
    "        b-=b.grad * 1e-5\n",
    "        w.grad.zero_()  #If this is not done, for second iteration , it would take double derivative - f'(f'(dl/dw)) or f''(dl/dw)\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRADIENT DESCENT ON 100 EPOCHS\n",
      "Loss at 99th iter tensor(318.1753, grad_fn=<DivBackward0>)\n",
      "Average loss in each element  tensor(17.8375, grad_fn=<SqrtBackward>)\n",
      "On average, each element in the prediction differs from the actual target by about 17.837467193603516\n"
     ]
    }
   ],
   "source": [
    "print(\"GRADIENT DESCENT ON 100 EPOCHS\")\n",
    "print(\"Loss at 99th iter\", loss)\n",
    "elementwise_loss = torch.sqrt(loss)\n",
    "print(\"Average loss in each element \",elementwise_loss)\n",
    "print(\"On average, each element in the prediction differs from the actual target by about\", elementwise_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochiastic Gradient descent : 1 example at a time\n",
    "### Basic tensor operations for back-propgation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Iteration1 / epoch 1: We take 1st example,random initialised value, calculate yhat, find loss(for 1st example y[0] - yhat[0]), find wgrad,bgrad and update weights\n",
    "We take 2nd example, calculate yhat, find loss(for 2nd example y[1] - yhat[1]), find wgrad,bgrad and update weights\n",
    "Similarly perform these tasks on all the examples\n",
    "\n",
    "#Iteration 2 / epoch 2: We take 1st example,random initialised value, calculate yhat, find loss(for 1st example y[0] - yhat[0]), find wgrad,bgrad and update weights\n",
    "We take 2nd example, calculate yhat, find loss(for 2nd example y[1] - yhat[1]), find wgrad,bgrad and update weights\n",
    "Similarly perform these tasks on all the example\n",
    "\n",
    "..... epoch 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iter  0  =  tensor(1.6428, grad_fn=<DivBackward0>)\n",
      "Loss at iter  0  =  tensor(2.5928, grad_fn=<DivBackward0>)\n",
      "Loss at iter  0  =  tensor(9.1601, grad_fn=<DivBackward0>)\n",
      "Loss at iter  0  =  tensor(0.7979, grad_fn=<DivBackward0>)\n",
      "Loss at iter  0  =  tensor(13.0677, grad_fn=<DivBackward0>)\n",
      "Loss at iter  10  =  tensor(1.6126, grad_fn=<DivBackward0>)\n",
      "Loss at iter  10  =  tensor(2.4272, grad_fn=<DivBackward0>)\n",
      "Loss at iter  10  =  tensor(8.2193, grad_fn=<DivBackward0>)\n",
      "Loss at iter  10  =  tensor(0.7543, grad_fn=<DivBackward0>)\n",
      "Loss at iter  10  =  tensor(11.8650, grad_fn=<DivBackward0>)\n",
      "Loss at iter  20  =  tensor(1.5844, grad_fn=<DivBackward0>)\n",
      "Loss at iter  20  =  tensor(2.2776, grad_fn=<DivBackward0>)\n",
      "Loss at iter  20  =  tensor(7.3740, grad_fn=<DivBackward0>)\n",
      "Loss at iter  20  =  tensor(0.7157, grad_fn=<DivBackward0>)\n",
      "Loss at iter  20  =  tensor(10.7803, grad_fn=<DivBackward0>)\n",
      "Loss at iter  30  =  tensor(1.5582, grad_fn=<DivBackward0>)\n",
      "Loss at iter  30  =  tensor(2.1422, grad_fn=<DivBackward0>)\n",
      "Loss at iter  30  =  tensor(6.6144, grad_fn=<DivBackward0>)\n",
      "Loss at iter  30  =  tensor(0.6816, grad_fn=<DivBackward0>)\n",
      "Loss at iter  30  =  tensor(9.8019, grad_fn=<DivBackward0>)\n",
      "Loss at iter  40  =  tensor(1.5336, grad_fn=<DivBackward0>)\n",
      "Loss at iter  40  =  tensor(2.0198, grad_fn=<DivBackward0>)\n",
      "Loss at iter  40  =  tensor(5.9323, grad_fn=<DivBackward0>)\n",
      "Loss at iter  40  =  tensor(0.6516, grad_fn=<DivBackward0>)\n",
      "Loss at iter  40  =  tensor(8.9192, grad_fn=<DivBackward0>)\n",
      "Loss at iter  50  =  tensor(1.5106, grad_fn=<DivBackward0>)\n",
      "Loss at iter  50  =  tensor(1.9092, grad_fn=<DivBackward0>)\n",
      "Loss at iter  50  =  tensor(5.3195, grad_fn=<DivBackward0>)\n",
      "Loss at iter  50  =  tensor(0.6252, grad_fn=<DivBackward0>)\n",
      "Loss at iter  50  =  tensor(8.1229, grad_fn=<DivBackward0>)\n",
      "Loss at iter  60  =  tensor(1.4892, grad_fn=<DivBackward0>)\n",
      "Loss at iter  60  =  tensor(1.8090, grad_fn=<DivBackward0>)\n",
      "Loss at iter  60  =  tensor(4.7692, grad_fn=<DivBackward0>)\n",
      "Loss at iter  60  =  tensor(0.6020, grad_fn=<DivBackward0>)\n",
      "Loss at iter  60  =  tensor(7.4045, grad_fn=<DivBackward0>)\n",
      "Loss at iter  70  =  tensor(1.4691, grad_fn=<DivBackward0>)\n",
      "Loss at iter  70  =  tensor(1.7183, grad_fn=<DivBackward0>)\n",
      "Loss at iter  70  =  tensor(4.2750, grad_fn=<DivBackward0>)\n",
      "Loss at iter  70  =  tensor(0.5817, grad_fn=<DivBackward0>)\n",
      "Loss at iter  70  =  tensor(6.7562, grad_fn=<DivBackward0>)\n",
      "Loss at iter  80  =  tensor(1.4503, grad_fn=<DivBackward0>)\n",
      "Loss at iter  80  =  tensor(1.6363, grad_fn=<DivBackward0>)\n",
      "Loss at iter  80  =  tensor(3.8313, grad_fn=<DivBackward0>)\n",
      "Loss at iter  80  =  tensor(0.5639, grad_fn=<DivBackward0>)\n",
      "Loss at iter  80  =  tensor(6.1713, grad_fn=<DivBackward0>)\n",
      "Loss at iter  90  =  tensor(1.4327, grad_fn=<DivBackward0>)\n",
      "Loss at iter  90  =  tensor(1.5620, grad_fn=<DivBackward0>)\n",
      "Loss at iter  90  =  tensor(3.4330, grad_fn=<DivBackward0>)\n",
      "Loss at iter  90  =  tensor(0.5485, grad_fn=<DivBackward0>)\n",
      "Loss at iter  90  =  tensor(5.6434, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    #print(\"At iteration \",i)\n",
    "    for j in range(len(x)):\n",
    "        yhat = model(x[j])\n",
    "        #print(yhat)\n",
    "        loss = loss_mse(y[j],yhat)\n",
    "        #print(y[j])\n",
    "        if(i % 10==0):\n",
    "            print(\"Loss at iter \" ,i ,\" = \",loss)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            w-=w.grad * 1e-5\n",
    "            b-=b.grad * 1e-5\n",
    "            w.grad.zero_()  #If this is not done, for second iteration , it would take double derivative - f'(f'(dl/dw)) or f''(dl/dw)\n",
    "            b.grad.zero_()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOCHIASTIC GRADIENT DESCENT RESULTS\n",
      "Loss at 99th iter tensor(5.2123, grad_fn=<DivBackward0>)\n",
      "Average loss in each element  tensor(2.2831, grad_fn=<SqrtBackward>)\n",
      "On average, each element in the prediction differs from the actual target by about 2.283052444458008\n"
     ]
    }
   ],
   "source": [
    "print(\"STOCHIASTIC GRADIENT DESCENT RESULTS\")\n",
    "print(\"Loss at 99th iter\", loss)\n",
    "elementwise_loss = torch.sqrt(loss)\n",
    "print(\"Average loss in each element \",elementwise_loss)\n",
    "print(\"On average, each element in the prediction differs from the actual target by about\", elementwise_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression using pytorch built-in functions -> Neural network lib(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 91.,  88.,  64.],\n",
       "        [102.,  43.,  37.]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
    "                   [102, 43, 37], [69, 96, 70], [72, 66, 42], \n",
    "                   [90, 87, 65], [85, 134, 56], [100, 42, 33], \n",
    "                   [68, 95, 74], [63, 77, 53], [81, 48, 54], \n",
    "                   [57, 124, 48], [142, 43, 37], [67, 96, 80],[50,70,30]],dtype='float32')\n",
    "targets = np.array([[56, 70], [81, 101], [119, 133], \n",
    "                    [22, 37], [103, 119], [66, 70], \n",
    "                    [84, 101], [120, 133], [22, 47], \n",
    "                    [103, 139], [59, 70], [81, 151], \n",
    "                    [129, 135], [23, 39], [104, 159],[50,60]], \n",
    "                   dtype='float32')\n",
    "x = torch.from_numpy(inputs)\n",
    "y = torch.from_numpy(targets)\n",
    "x[[1,3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 examples -> create batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TensorDataset -> give input as output as tuple. Useful for taking small samples of data\n",
    "from torch.utils.data import TensorDataset\n",
    "train_ds = TensorDataset(x,y)\n",
    "train_ds[:3]\n",
    "#train_ds[[1,3]] #1st tensor is input, 2nd tensor is the target(y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DataLoader:\n",
    "Split data into batches -> Earlier in python we would use ,no of batches = total_examples/batchsize.\n",
    "Say, batchsize =4, i.e 4 training examples considered in 1 run, here, total_examples= no_of_batches = 16/4=4batches\n",
    "All datapoints will be considered, the tuple will not be repeated/duplicated in the batches\n",
    "This operation can be achieved using DataLoader\n",
    "\n",
    "We can do shuffling and random sampling. \n",
    "input training set would be (x,y) tuple obtained from TensorDataset(train_ds)\n",
    "\n",
    "Shuffling helps randomize the input to the optimization algorithm, which can lead to faster reduction in the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_dl = DataLoader(train_ds,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 50.,  70.,  30.],\n",
      "        [100.,  42.,  33.],\n",
      "        [ 63.,  77.,  53.]])\n",
      "tensor([[56., 70.],\n",
      "        [50., 60.],\n",
      "        [22., 47.],\n",
      "        [59., 70.]])\n"
     ]
    }
   ],
   "source": [
    "for xb,yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 ITERATION, CONSIDERING ALL EXAMPLES - GRADIENT DESCENT\n",
    "### Using pytorch in-built optimizer (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "torch.manual_seed(3)\n",
    "model = nn.Linear(3,2)\n",
    "#print(\"Weight :: \",model.weight)\n",
    "#print(\"Bias :: \",model.bias)\n",
    "loss_fn =  F.mse_loss\n",
    "opt = torch.optim.SGD(list(model.parameters()), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.4348, -0.3015, -0.1514],\n",
      "        [-0.4091,  0.1158, -0.4122]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3157, 0.2831], requires_grad=True)]\n",
      "Loss 22216.19140625\n",
      "Average loss in each element  149.05096915568848\n",
      "On average, each element in the prediction differs from the actual target by about 149.05096915568848\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters())) # weights and bias matrices are the parameters\n",
    "yhat = model(x)\n",
    "loss = loss_fn(yhat,y)\n",
    "loss.backward()\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "print(\"Loss\",loss.item())\n",
    "elementwise_loss = np.sqrt(loss.item())\n",
    "print(\"Average loss in each element \",elementwise_loss)\n",
    "print(\"On average, each element in the prediction differs from the actual target by about\", elementwise_loss.item())\n",
    "#print(list(model.parameters())) #UPDATED WEIGHTS AND BIASES AFTER OPTIMISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100 ITERATIONS(EPOCHS), CONSIDERING ALL EXAMPLES - GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "torch.manual_seed(2) #We do this before creating model, since, model initialises the w, b matrices randomly\n",
    "model = nn.Linear(3,2)\n",
    "loss_fn = F.mse_loss\n",
    "opt = torch.optim.SGD(model.parameters(), lr= 1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At each epoch -> \n",
    "# we consider all 16 examples , predict output, calculate loss , grad descent and update the weights. \n",
    "# Thus, at each epoch we update the weights and bias matrix just once.\n",
    "# With 100 epoch we will have 100 *1 = 100 losses calculated i.e weights will be updated 100 times (counter)\n",
    "\n",
    "\n",
    "def gradDes(model, opt,loss_fn, x, epochs,counter=0):\n",
    "    for i in range(epochs):\n",
    "        counter=counter+1\n",
    "        # model would have initialised w,b matrices. Predict the output considering input(X) and initialised w,b\n",
    "        pred =yhat= model(x)\n",
    "        #Find the loss from defined mse loss function. Pass actual and predicted values\n",
    "        loss = loss_fn(yhat,y)\n",
    "        #print(loss)\n",
    "        # Perform backward/grad_des/differentiation on loss function\n",
    "        loss.backward()\n",
    "        \n",
    "        #Optimisation : Update the w and b based on on grad descent calculation. grad optimizer takes care of this\n",
    "        opt.step()\n",
    "        \n",
    "        # set calculated grad values of w, b to zero\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        #if (i+1) % 10 == 0:\n",
    "            #print(\"Loss at epoch {} is {:.4f}\".format(i,loss.item()))\n",
    "        \n",
    "    return (loss.item(),counter)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter 100\n",
      "Loss at 99th iter 6332.97216796875\n",
      "Average loss in each element  79.57997341020383\n",
      "On average, each element in the prediction differs from the actual target by about 79.57997341020383\n"
     ]
    }
   ],
   "source": [
    "loss,counter = gradDes(model,opt,loss_fn,x,100)\n",
    "print(\"counter\",counter)\n",
    "print(\"Loss at 99th iter\", loss)\n",
    "elementwise_loss = np.sqrt(loss)\n",
    "print(\"Average loss in each element \",elementwise_loss)\n",
    "print(\"On average, each element in the prediction differs from the actual target by about\", elementwise_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100 ITERATIONS, CONSIDERING 1 EXAMPLE AT A TIME - STOCHIASTIC GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "torch.manual_seed(2) #We do this before creating model, since, model initialises the w, b matrices randomly\n",
    "model = nn.Linear(3,2)\n",
    "loss_fn = F.mse_loss\n",
    "opt = torch.optim.SGD(model.parameters(), lr= 1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At each epoch -> \n",
    "# first we consider 1 example and update the weights.\n",
    "# next consider 2nd example  and updated weights and calculate loss and update weights and so on\n",
    "# This continus for all 16 examples.Thus each epoch will have 16 losses calculated\n",
    "# With 100 epoch we will have 100 *16= 1600 losses calculated i.e weights will be updated 1600 times (counter)\n",
    "\n",
    "def stochGradDes(model, opt,loss_fn, x, epochs,counter=0):\n",
    "    for i in range(epochs):\n",
    "        for j in range(len(train_ds)):\n",
    "            counter= counter+1\n",
    "        # model would have initialised w,b matrices. Predict the output considering input(X) and initialised w,b\n",
    "            pred = yhat= model(x[j])\n",
    "        \n",
    "        #Find the loss from defined mse loss function. Pass actual and predicted values\n",
    "            loss = loss_fn(yhat,y[j])\n",
    "        \n",
    "        # Perform backward/grad_des/differentiation on loss function\n",
    "            loss.backward()\n",
    "        \n",
    "        #Optimisation : Update the w and b based on on grad descent calculation. grad optimizer takes care of this\n",
    "            opt.step()\n",
    "        \n",
    "        # set calculated grad values of w, b to zero\n",
    "            opt.zero_grad()\n",
    "\n",
    "            #if (i+1) % 10 == 0:\n",
    "                #print(\"Loss at epoch {} is {:.4f}\".format(i,loss.item()))\n",
    "        \n",
    "    return (loss.item(),counter)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 99th iter 1989.31591796875\n",
      "Counter =  1600\n",
      "Average loss in each element  44.60174792503933\n",
      "On average, each element in the prediction differs from the actual target by about 44.60174792503933\n"
     ]
    }
   ],
   "source": [
    "(loss,counter) = stochGradDes(model,opt,loss_fn,x,100)\n",
    "print(\"Loss at 99th iter\", loss)\n",
    "print(\"Counter = \",counter)\n",
    "elementwise_loss = np.sqrt(loss)\n",
    "print(\"Average loss in each element \",elementwise_loss)\n",
    "print(\"On average, each element in the prediction differs from the actual target by about\", elementwise_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100 ITERATIONS, CONSIDERING 4 EXAMPLES AT A TIME - BATCH GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "torch.manual_seed(2) #We do this before creating model, since, model initialises the w, b matrices randomly\n",
    "model = nn.Linear(3,2)\n",
    "loss_fn = F.mse_loss\n",
    "opt = torch.optim.SGD(model.parameters(), lr= 1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At each epoch -> \n",
    "# first we consider 4 examples, predict the outputs for the 4 examples, calculate overall loss(sum loss for all examples)\n",
    "# and perform grad descent and update the weights.\n",
    "# next consider next 4 examples calculate loss and updated weights  so on\n",
    "# This continues for all 16 examples i.e 4 batches -> each batch containing 4 examples shuffled(train_dl).Thus each epoch will have 4 losses calculated\n",
    "# With 100 epoch we will have 100 *4= 400 losses calculated i.e weights will be updated 400 times\n",
    "\n",
    "\n",
    "def batchGradDes(model, opt,loss_fn, x, epochs,counter=0):\n",
    "    for i in range(epochs):\n",
    "        for xb,yb in train_dl:\n",
    "            counter = counter+1\n",
    "        # model would have initialised w,b matrices. Predict the output considering input(X) and initialised w,b\n",
    "            pred = yhat= model(xb)\n",
    "        #Find the loss from defined mse loss function. Pass actual and predicted values\n",
    "            loss = loss_fn(yhat,yb)\n",
    "\n",
    "        # Perform backward/grad_des/differentiation on loss function\n",
    "            loss.backward()\n",
    "        \n",
    "        #Optimisation : Update the w and b based on on grad descent calculation. grad optimizer takes care of this\n",
    "            opt.step()\n",
    "        \n",
    "        # set calculated grad values of w, b to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "            #if (i+1) % 10 == 0:\n",
    "                #print(\"Loss at epoch {} is {:.4f}\".format(i,loss.item()))\n",
    "        \n",
    "    return (loss.item(),counter)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 99th iter 4408.11865234375\n",
      "counter 400\n",
      "Average loss in each element  66.39366424850905\n",
      "On average, each element in the prediction differs from the actual target by about 66.39366424850905\n"
     ]
    }
   ],
   "source": [
    "(loss,counter) = batchGradDes(model,opt,loss_fn,x,100)\n",
    "print(\"Loss at 99th iter\", loss)\n",
    "print(\"counter\",counter)\n",
    "elementwise_loss = np.sqrt(loss)\n",
    "print(\"Average loss in each element \",elementwise_loss)\n",
    "print(\"On average, each element in the prediction differs from the actual target by about\", elementwise_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 iteration, all examples\n",
    "Overall Loss = 22216.19140625 . \n",
    "On average, each element in the prediction differs from the actual target by about 149.05096915568848\n",
    "\n",
    "\n",
    "### 100 iterations all examples\n",
    "Overall Loss at 99th iter 6332.97216796875 .\n",
    "On average, each element in the prediction differs from the actual target by about 79.57997341020383\n",
    "\n",
    "### 100 iterations 1 example at a time\n",
    "Overall Loss at 99th iter 1989.31591796875 .\n",
    "On average, each element in the prediction differs from the actual target by about 44.60174792503933\n",
    "\n",
    "### 100 iterations with batch_size=4, i.e considering 4 examples at a time\n",
    "Overall Loss at 99th iter 4408.11865234375.\n",
    "On average, each element in the prediction differs from the actual target by about 66.39366424850905\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
